{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import blobfile as bf\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "file_dir = os.path.abspath('')\n",
    "\n",
    "data_dir = os.path.join(file_dir, '..','data_preprocessing','generated_data')\n",
    "\n",
    "guided_diff_path = os.path.join(file_dir, '..', '..','re_design', 'guided-diffusion')\n",
    "if guided_diff_path not in sys.path:\n",
    "    sys.path.append(guided_diff_path)\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "\n",
    "train_utils_path = os.path.join(file_dir,'..','train_utils')\n",
    "if train_utils_path not in sys.path:\n",
    "    sys.path.append(train_utils_path)\n",
    "from utils_data import load_TF_data, SequenceDataset\n",
    "from guided_tools import (\n",
    "    get_data_generator,\n",
    "    TrainLoop,\n",
    "    create_model,\n",
    "    create_gaussian_diffusion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2024-07-10-20-59-08-351589\n"
     ]
    }
   ],
   "source": [
    "class DiffusionTrainingconfig:\n",
    "    data_dir=f'{data_dir}/tcre_seq_motif_cluster.csv'\n",
    "    schedule_sampler=\"uniform\"\n",
    "    lr=1e-4\n",
    "    weight_decay=0.0\n",
    "    lr_anneal_steps=0\n",
    "    batch_size=16\n",
    "    microbatch=-1  # -1 disables microbatches\n",
    "    ema_rate=\"0.9999\"  # comma-separated list of EMA values\n",
    "    log_interval=10\n",
    "    sample_interval=10000\n",
    "    save_interval=10000\n",
    "    resume_checkpoint=\"\"\n",
    "    use_fp16=False\n",
    "    fp16_scale_growth=1e-3\n",
    "\n",
    "    subset = None\n",
    "    num_workers = 1\n",
    "    num_sampling_to_compare_cells = 1000\n",
    "    kmer_length = 5\n",
    "    num_classes = 3\n",
    "    \n",
    "\n",
    "    # model\n",
    "    seq_length=200\n",
    "    image_size=seq_length\n",
    "    \n",
    "    num_channels=128\n",
    "    num_res_blocks=2\n",
    "    num_heads=4\n",
    "    num_heads_upsample=-1\n",
    "    num_head_channels=-1\n",
    "    attention_resolutions=\"16,8\"\n",
    "    channel_mult=\"\"\n",
    "    dropout=0.0\n",
    "    class_cond=True\n",
    "    use_checkpoint=False\n",
    "    use_scale_shift_norm=True\n",
    "    resblock_updown=True\n",
    "    use_fp16=False\n",
    "    use_new_attention_order=False\n",
    "\n",
    "    # Diffusion\n",
    "    learn_sigma=False\n",
    "    diffusion_steps=1000\n",
    "    noise_schedule=\"linear\"\n",
    "    timestep_respacing=\"\"\n",
    "    use_kl=False\n",
    "    predict_xstart=False\n",
    "    rescale_timesteps=False\n",
    "    rescale_learned_sigmas=False\n",
    "\n",
    "    # Sampling\n",
    "    use_ddim=False\n",
    "    clip_denoised=True\n",
    "    sample_bs = 100\n",
    "\n",
    "\n",
    "config = DiffusionTrainingconfig()\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data loader...\n"
     ]
    }
   ],
   "source": [
    "logger.log(\"creating data loader...\")\n",
    "data = load_TF_data(\n",
    "    data_path=config.data_dir,\n",
    "    seqlen=config.seq_length,\n",
    "    limit_total_sequences=config.subset,\n",
    "    num_sampling_to_compare_cells=config.num_sampling_to_compare_cells,\n",
    "    to_save_file_name=\"cre_encode_data_motif_cluster\",\n",
    "    saved_file_name=\"cre_encode_data_motif_cluster.pkl\",\n",
    "    load_saved_data=True,\n",
    "    start_label_number = 0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model and diffusion...\n"
     ]
    }
   ],
   "source": [
    "logger.log(\"creating model and diffusion...\")\n",
    "model = create_model(\n",
    "        config.image_size,\n",
    "        config.num_channels,\n",
    "        config.num_res_blocks,\n",
    "        channel_mult=config.channel_mult,\n",
    "        learn_sigma=config.learn_sigma,\n",
    "        class_cond=config.class_cond,\n",
    "        use_checkpoint=config.use_checkpoint,\n",
    "        attention_resolutions=config.attention_resolutions,\n",
    "        num_heads=config.num_heads,\n",
    "        num_head_channels=config.num_head_channels,\n",
    "        num_heads_upsample=config.num_heads_upsample,\n",
    "        use_scale_shift_norm=config.use_scale_shift_norm,\n",
    "        dropout=config.dropout,\n",
    "        resblock_updown=config.resblock_updown,\n",
    "        use_fp16=config.use_fp16,\n",
    "        use_new_attention_order=config.use_new_attention_order,\n",
    "    )\n",
    "diffusion = create_gaussian_diffusion(\n",
    "        steps=config.diffusion_steps,\n",
    "        learn_sigma=config.learn_sigma,\n",
    "        noise_schedule=config.noise_schedule,\n",
    "        use_kl=config.use_kl,\n",
    "        predict_xstart=config.predict_xstart,\n",
    "        rescale_timesteps=config.rescale_timesteps,\n",
    "        rescale_learned_sigmas=config.rescale_learned_sigmas,\n",
    "        timestep_respacing=config.timestep_respacing,\n",
    "    )\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "schedule_sampler = create_named_schedule_sampler(config.schedule_sampler, diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(\"training...\")\n",
    "TrainLoop(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    data=data,\n",
    "    wandb_logging=False,\n",
    "    batch_size=config.batch_size,\n",
    "    microbatch=config.microbatch,\n",
    "    lr=config.lr,\n",
    "    ema_rate=config.ema_rate,\n",
    "    log_interval=config.log_interval,\n",
    "    sample_interval = config.sample_interval,\n",
    "    save_interval=config.save_interval,\n",
    "    resume_checkpoint=config.resume_checkpoint,\n",
    "    use_fp16=config.use_fp16,\n",
    "    fp16_scale_growth=config.fp16_scale_growth,\n",
    "    schedule_sampler=schedule_sampler,\n",
    "    weight_decay=config.weight_decay,\n",
    "    lr_anneal_steps=config.lr_anneal_steps,\n",
    ").run_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample_fn(\n\u001b[1;32m     21\u001b[0m     model,\n\u001b[1;32m     22\u001b[0m     (config\u001b[38;5;241m.\u001b[39msample_bs, \u001b[38;5;241m4\u001b[39m, config\u001b[38;5;241m.\u001b[39mimage_size),\n\u001b[1;32m     23\u001b[0m     clip_denoised\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclip_denoised,\n\u001b[1;32m     24\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m sample \u001b[38;5;241m=\u001b[39m ((sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m127.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m---> 27\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     30\u001b[0m gathered_samples \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros_like(sample) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dist\u001b[38;5;241m.\u001b[39mget_world_size())]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "model.to(dist_util.dev())\n",
    "if config.use_fp16:\n",
    "    model.convert_to_fp16()\n",
    "model.eval()\n",
    "\n",
    "logger.log(\"sampling...\")\n",
    "all_images = []\n",
    "all_labels = []\n",
    "while len(all_images) * config.sample_bs < config.num_sampling_to_compare_cells:\n",
    "    model_kwargs = {}\n",
    "    if config.class_cond:\n",
    "        classes = torch.randint(\n",
    "            low=0, high=NUM_CLASSES, size=(config.sample_bs,), device=dist_util.dev()\n",
    "        )\n",
    "        model_kwargs[\"y\"] = classes\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not config.use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        (config.sample_bs, 4, config.image_size),\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "    sample = ((sample + 1) * 127.5).clamp(0, 255).to(torch.uint8)\n",
    "    sample = sample.permute(0, 2, 3, 1)\n",
    "    sample = sample.contiguous()\n",
    "\n",
    "    gathered_samples = [torch.zeros_like(sample) for _ in range(dist.get_world_size())]\n",
    "    dist.all_gather(gathered_samples, sample)  # gather not supported with NCCL\n",
    "    all_images.extend([sample.cpu().numpy() for sample in gathered_samples])\n",
    "    if config.class_cond:\n",
    "        gathered_labels = [\n",
    "            torch.zeros_like(classes) for _ in range(dist.get_world_size())\n",
    "        ]\n",
    "        dist.all_gather(gathered_labels, classes)\n",
    "        all_labels.extend([labels.cpu().numpy() for labels in gathered_labels])\n",
    "    logger.log(f\"created {len(all_images) * config.sample_bs} samples\")\n",
    "\n",
    "arr = np.concatenate(all_images, axis=0)\n",
    "arr = arr[: config.num_sampling_to_compare_cells]\n",
    "if config.class_cond:\n",
    "    label_arr = np.concatenate(all_labels, axis=0)\n",
    "    label_arr = label_arr[: config.num_sampling_to_compare_cells]\n",
    "if dist.get_rank() == 0:\n",
    "    shape_str = \"x\".join([str(x) for x in arr.shape])\n",
    "    out_path = os.path.join(logger.get_dir(), f\"samples_{shape_str}.npz\")\n",
    "    logger.log(f\"saving to {out_path}\")\n",
    "    if config.class_cond:\n",
    "        np.savez(out_path, arr, label_arr)\n",
    "    else:\n",
    "        np.savez(out_path, arr)\n",
    "\n",
    "dist.barrier()\n",
    "logger.log(\"sampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {}\n",
    "if config.class_cond:\n",
    "    classes = torch.randint(\n",
    "        low=0, high=NUM_CLASSES, size=(config.sample_bs,), device=dist_util.dev()\n",
    "    )\n",
    "    model_kwargs[\"y\"] = classes\n",
    "sample_fn = (\n",
    "    diffusion.p_sample_loop if not config.use_ddim else diffusion.ddim_sample_loop\n",
    ")\n",
    "sample = sample_fn(\n",
    "    model,\n",
    "    (config.sample_bs, 4, config.image_size),\n",
    "    clip_denoised=config.clip_denoised,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cell_types=data[\"cell_types\"]\n",
    "# count cell types in train\n",
    "cell_dict_temp = Counter(data['x_train_cell_type'].tolist())\n",
    "# reoder by cell_types list\n",
    "cell_dict = {k:cell_dict_temp[k] for k in data['cell_types']}\n",
    "# take only counts\n",
    "cell_type_counts = list(cell_dict.values())\n",
    "cell_type_probabilities = [x / sum(cell_type_counts) for x in cell_type_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n",
    "final_sequences = []\n",
    "plain_generated_sequences = []\n",
    "for n_a in range(int(config.num_sampling_to_compare_cells/ config.sample_bs)):\n",
    "    model_kwargs = {}\n",
    "    if config.class_cond:\n",
    "        classes = torch.randint(\n",
    "            low=0, high=NUM_CLASSES, size=(config.sample_bs,), device=dist_util.dev()\n",
    "        )\n",
    "        model_kwargs[\"y\"] = classes\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not config.use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "    sampled_images = sample_fn(\n",
    "        model,\n",
    "        (config.sample_bs, 4, config.image_size),\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "    \n",
    "    for n_b, x in enumerate(sampled_images):\n",
    "        sequence = \"\".join([nucleotides[s] for s in np.argmax(x.detach().cpu(), axis=0)])\n",
    "        plain_generated_sequences.append(sequence)\n",
    "        seq_final = f\">seq_test_{n_a}_{n_b}\\n\" + sequence\n",
    "        final_sequences.append(seq_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n",
    "final_sequences = []\n",
    "plain_generated_sequences = []\n",
    "for n_b, x in enumerate(sample):\n",
    "    sequence = \"\".join([nucleotides[s] for s in np.argmax(x.detach().cpu(), axis=0)])\n",
    "    plain_generated_sequences.append(sequence)\n",
    "    seq_final = f\">seq_test_{n_b}\\n\" + sequence\n",
    "    final_sequences.append(seq_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
